{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Interface | Advantage : ETL in SQL and not in pandas\n",
    "# https://colab.research.google.com/drive/1BNRu6ZS0HAlg4c2ls-UfGGkKD8bLAZ0T?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_data = openml.datasets.get_dataset(40945)\n",
    "\n",
    "# Get the data and target separately\n",
    "X, y, _, _ = titanic_data.get_data(target=titanic_data.default_target_attribute)\n",
    "\n",
    "# Get the feature names from the Titanic dataset\n",
    "attribute_names = titanic_data.features.values()\n",
    "feature_names = [feat.name for feat in attribute_names if feat.name != titanic_data.default_target_attribute]\n",
    "\n",
    "titanic_df = pd.DataFrame(X, columns=feature_names)\n",
    "titanic_df['Survived'] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SQLite connection in memory\n",
    "conn=sqlite3.connect(':memory:')\n",
    "\n",
    "# Write the data into db table\n",
    "titanic_df.to_sql('titanic',conn,if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='SELECT * FROM titanic LIMIT 10;'\n",
    "pd.read_sql_query(query,conn) #output is a pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIPELINE (Auto ETL) | NOTE the difference steps for categorical and numerical\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define preprocessing steps\n",
    "numeric_features = ['age', 'fare']\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_features = ['pclass', 'sex', 'embarked']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# Preprocess the data\n",
    "X_preprocessed = preprocessor.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_preprocessed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy:.2f}\") # means model is able to predict 77% accuracy whether the person survived or dead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# from IPython.display import display\n",
    "\n",
    "# def predict_survival(pclass, sex, age, fare, embarked):\n",
    "#     # Create a DataFrame for the input\n",
    "#     input_data = pd.DataFrame(\n",
    "#         [[pclass, sex, age, fare, embarked]],\n",
    "#         columns=['pclass', 'sex', 'age', 'fare', 'embarked']\n",
    "#     )\n",
    "    \n",
    "#     # Preprocess the input\n",
    "#     input_preprocessed = preprocessor.transform(input_data)\n",
    "    \n",
    "#     # Make a prediction\n",
    "#     prediction = model.predict(input_preprocessed)\n",
    "    \n",
    "#     if prediction[0] == 1:\n",
    "#         print(\"The passenger is predicted to survive.\")\n",
    "#     else:\n",
    "#         print(\"The passenger is predicted to perish.\")\n",
    "\n",
    "# # Create widgets for user input\n",
    "# pclass_widget = widgets.IntSlider(min=1, max=3, step=1, value=1, description='Pclass:')\n",
    "# sex_widget = widgets.Dropdown(options=['male', 'female'], value='male', description='Sex:')\n",
    "# age_widget = widgets.FloatSlider(min=0, max=100, step=1, value=30, description='Age:')\n",
    "# fare_widget = widgets.FloatSlider(min=0, max=600, step=1, value=50, description='Fare:')\n",
    "# embarked_widget = widgets.Dropdown(options=['C', 'Q', 'S'], value='S', description='Embarked:')\n",
    "\n",
    "# # Display the widgets and bind them to the predict_survival function\n",
    "# widgets.interact(predict_survival, pclass=pclass_widget, sex=sex_widget, age=age_widget, fare=fare_widget, embarked=embarked_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/drive/1ONMhTMJU2M2FaHWtppSWwqPAqQJNMTdw#scrollTo=o9uPme0QdZGk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# what is the difference between fit and transform\n",
    "\n",
    "In scikit-learn, fit() and transform() are two separate methods used for centering/feature scaling of a given dataset [0]. The fit() method calculates the parameters (e.g. mean and standard deviation) of the dataset and saves them as internal objects [2]. On the other hand, the transform() method applies these calculated parameters to the dataset to generate transformed data [0].\n",
    "\n",
    "The fit_transform() method is a combination of both fit() and transform() methods on the same dataset. It is used for the initial fitting of parameters on the training set, while also returning the transformed dataset. Internally, the transformer object first calls fit() and then transform() on the same data [2]. This method is used to normalize the data within a particular range and help in feature scaling [0].\n",
    "\n",
    "The fit() method is used for generating learning model parameters from training data [0]. It is used to compute the mean and standard deviation for a given feature to be used further for scaling [3]. When applying the fit() method to a dataset, it learns from the data and calculates the required parameters [1].\n",
    "\n",
    "The transform() method, on the other hand, applies the learned parameters to the dataset to generate transformed data set [0]. It is used to perform scaling using the mean and standard deviation calculated using the fit() method [3]. The transform() method is normally used on the test data and unseen data in general [1].\n",
    "\n",
    "It is important to note that fit_transform() should only be applicable to the training data, and not to the test data. This is because the test data should use the same parameters (e.g. mean and standard deviation) calculated from the training data set [0]. Using fit_transform() on the test data is a common rookie mistake [1].\n",
    "\n",
    "In summary, the main difference between fit() and transform() in scikit-learn is that fit() is used for generating learning model parameters from training data, while transform() applies the learned parameters to the dataset to generate transformed data set. fit_transform() is a combination of both fit() and transform() methods on the same dataset, used to normalize the data within a particular range and help in feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
