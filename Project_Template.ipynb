{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/gitkundan/mle/blob/main/Project_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WofvObKecvG_"
   },
   "source": [
    "# New section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UyXUkJscrBQ"
   },
   "source": [
    "High Level Steps:\n",
    "\n",
    "1. Import Modules,set environment\n",
    "2. Load dataset\n",
    "3. \n",
    "\n",
    "2.   List item\n",
    "\n",
    "\n",
    "1.   List item\n",
    "2.   List item\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "KZAkFoZEcK5R"
   },
   "outputs": [],
   "source": [
    "#1. Import Modules, set environment\n",
    "# a) Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import missingno as msno\n",
    "import ssl \n",
    "import certifi\n",
    "from sklearn.datasets import fetch_openml \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "UEe9WYRYdTgW",
    "outputId": "145cd71c-aa73-403d-c9b5-0ecdf6b43910"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9bbc9b29-c3bc-4bb7-bbc8-8b39558fd463\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_age</th>\n",
       "      <th>person_income</th>\n",
       "      <th>person_home_ownership</th>\n",
       "      <th>person_emp_length</th>\n",
       "      <th>loan_intent</th>\n",
       "      <th>loan_grade</th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>loan_int_rate</th>\n",
       "      <th>loan_status</th>\n",
       "      <th>loan_percent_income</th>\n",
       "      <th>cb_person_default_on_file</th>\n",
       "      <th>cb_person_cred_hist_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2244</th>\n",
       "      <td>25</td>\n",
       "      <td>31000</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>4.0</td>\n",
       "      <td>VENTURE</td>\n",
       "      <td>B</td>\n",
       "      <td>10000</td>\n",
       "      <td>10.38</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>N</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7655</th>\n",
       "      <td>24</td>\n",
       "      <td>47480</td>\n",
       "      <td>RENT</td>\n",
       "      <td>8.0</td>\n",
       "      <td>MEDICAL</td>\n",
       "      <td>B</td>\n",
       "      <td>6900</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>N</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21710</th>\n",
       "      <td>34</td>\n",
       "      <td>50000</td>\n",
       "      <td>RENT</td>\n",
       "      <td>5.0</td>\n",
       "      <td>PERSONAL</td>\n",
       "      <td>B</td>\n",
       "      <td>6000</td>\n",
       "      <td>9.91</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>N</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16000</th>\n",
       "      <td>26</td>\n",
       "      <td>42000</td>\n",
       "      <td>RENT</td>\n",
       "      <td>2.0</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>C</td>\n",
       "      <td>4800</td>\n",
       "      <td>14.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0.11</td>\n",
       "      <td>Y</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25499</th>\n",
       "      <td>29</td>\n",
       "      <td>85000</td>\n",
       "      <td>MORTGAGE</td>\n",
       "      <td>2.0</td>\n",
       "      <td>HOMEIMPROVEMENT</td>\n",
       "      <td>A</td>\n",
       "      <td>10000</td>\n",
       "      <td>7.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>N</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9bbc9b29-c3bc-4bb7-bbc8-8b39558fd463')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9bbc9b29-c3bc-4bb7-bbc8-8b39558fd463 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9bbc9b29-c3bc-4bb7-bbc8-8b39558fd463');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "       person_age  person_income person_home_ownership  person_emp_length  \\\n",
       "2244           25          31000              MORTGAGE                4.0   \n",
       "7655           24          47480                  RENT                8.0   \n",
       "21710          34          50000                  RENT                5.0   \n",
       "16000          26          42000                  RENT                2.0   \n",
       "25499          29          85000              MORTGAGE                2.0   \n",
       "\n",
       "           loan_intent loan_grade  loan_amnt  loan_int_rate  loan_status  \\\n",
       "2244           VENTURE          B      10000          10.38            0   \n",
       "7655           MEDICAL          B       6900           9.88            0   \n",
       "21710         PERSONAL          B       6000           9.91            0   \n",
       "16000        EDUCATION          C       4800          14.27            1   \n",
       "25499  HOMEIMPROVEMENT          A      10000           7.88            0   \n",
       "\n",
       "       loan_percent_income cb_person_default_on_file  \\\n",
       "2244                  0.32                         N   \n",
       "7655                  0.15                         N   \n",
       "21710                 0.12                         N   \n",
       "16000                 0.11                         Y   \n",
       "25499                 0.12                         N   \n",
       "\n",
       "       cb_person_cred_hist_length  \n",
       "2244                            4  \n",
       "7655                            2  \n",
       "21710                           8  \n",
       "16000                           2  \n",
       "25499                          10  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load Dataset\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# Download the dataset from openml\n",
    "# dataset = fetch_openml(data_id=42803, as_frame=True) #Accident\n",
    "dataset = fetch_openml(data_id=43454, as_frame=True) #Credit risk\n",
    "\n",
    "#Extract feature matrix X and show 5 random samples\n",
    "df = dataset[\"frame\"]\n",
    "df.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "1xbxCLz8dU31",
    "outputId": "5606e2b8-eb44-4a89-8590-46afcfea2c04"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"\\n3. Structural Analysis:\\n(a) basic queries : sample, info, describe, shape\\n(b) Cast to correct data types\\ne.g. df['measles_'] = df['measles_'].astype('int64')\\n(c) Treat obviously inocorrect values\\n\\n# If you have values in your columns that you want to replace - use this for loop\\n# For instance - in the CreditScoring dataset - there are numerous 99999999 that need to be replaced\\n\\n# for c in ['income', 'assets', 'debt']:\\n#    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\\n\\n\\n# If you want to remove a value from a column - use this: \\n# df = df[df.status != 'unk']   # This removes the value 'unk' from your data in the column.  Modify as needed\\n\\n# drop multiple columns by name\\n# df.drop(['column_name1', 'column_name2'], axis=1, inplace=True)\\n\\npd.value_counts(df.dtypes)\\n\\ndf.nunique().to_frame()\\n#identify what is categorical\\n# identify too many value in categorical - OHE will turn into columns\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "3. Structural Analysis:\n",
    "(a) basic queries : sample, info, describe, shape\n",
    "(b) Cast to correct data types\n",
    "e.g. df['measles_'] = df['measles_'].astype('int64')\n",
    "(c) Treat obviously inocorrect values\n",
    "\n",
    "# If you have values in your columns that you want to replace - use this for loop\n",
    "# For instance - in the CreditScoring dataset - there are numerous 99999999 that need to be replaced\n",
    "\n",
    "# for c in ['income', 'assets', 'debt']:\n",
    "#    df[c] = df[c].replace(to_replace=99999999, value=np.nan)\n",
    "\n",
    "\n",
    "# If you want to remove a value from a column - use this: \n",
    "# df = df[df.status != 'unk']   # This removes the value 'unk' from your data in the column.  Modify as needed\n",
    "\n",
    "# drop multiple columns by name\n",
    "# df.drop(['column_name1', 'column_name2'], axis=1, inplace=True)\n",
    "\n",
    "pd.value_counts(df.dtypes)\n",
    "\n",
    "df.nunique().to_frame()\n",
    "#identify what is categorical\n",
    "# identify too many value in categorical - OHE will turn into columns\n",
    "\n",
    "\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "Ha9f_3tPfZ_9",
    "outputId": "a11aedd9-2612-4304-e568-d6fdd2d149d9"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-803b6acfff13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Check number of duplicates while ignoring the index feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mn_duplicates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accident_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You seem to have {n_duplicates} duplicates in your database.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4904\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4905\u001b[0m         \"\"\"\n\u001b[0;32m-> 4906\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   4907\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4908\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4149\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4150\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   4183\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4184\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4185\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4186\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6015\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6017\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6018\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6019\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['accident_index'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# 3. Qualitative Investigation of the Data\n",
    "# a) Duplicates\n",
    "# Duplicates in the Columns?\n",
    "df.duplicated()\n",
    "df.duplicated().sum()\n",
    "\n",
    "# Duplicated Rows?\n",
    "df[df.duplicated()]\n",
    "# Find duplicates in specific columns from your dataset.  Replace with your columns obviously.  keep the first or last dupe.\n",
    "# df.loc[df.duplicated(keep='last'), ['InvoiceNo', 'StockCode', 'InvoiceDate', 'CustomerID']]\n",
    "\n",
    "# Only consider duplicates in these columns and drop only them\n",
    "# df.duplicated(subset=['InvoiceNo', 'StockCode', 'InvoiceDate','CustomerID'], keep='first').sum()\n",
    "# By looking only at these four columns instead of all of them, we can see that the number of duplicate rows may increase/decrease\n",
    "# This means that there are rows that have the exact same values as these four columns but have different values in \n",
    "# other columns, which means they may be different records. \n",
    "# In most cases, it is better to use all the columns to identify duplicate records.\n",
    "\n",
    "# df_unique = df.drop_duplicates(keep='first')\n",
    "# df.drop_duplicates(keep='first')\n",
    "\n",
    "# Check number of duplicates while ignoring the index feature\n",
    "n_duplicates = df.drop(labels=['accident_index'], axis=1).duplicated().sum()\n",
    "\n",
    "print(f\"You seem to have {n_duplicates} duplicates in your database.\")\n",
    "\n",
    "#  Extract column names of all features, except 'Accident_Index'\n",
    "columns_to_consider = df.drop(labels=['accident_index'], axis=1).columns\n",
    "\n",
    "# Drop duplicates based on 'columns_to_consider'\n",
    "df.drop_duplicates(subset=columns_to_consider, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m5hI8csLfc4v"
   },
   "outputs": [],
   "source": [
    "# MISSING VALUES\n",
    "df.isna()\n",
    "df.isna().sum()\n",
    "\n",
    "# Check for missing values in a single column\n",
    "# df[df['Description'].isna()]\n",
    "\n",
    "# List all rows that are missing a value in this field\n",
    "# df.dropna(subset=['Description'])  \n",
    "\n",
    "# Drop all rows that are missing a value in this field:\n",
    "# df.dropna(subset=['Description'], inplace=True)\n",
    "\n",
    "# b) Missing Values per Sample (Big Holes)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "g = sns.heatmap(df.isnull(), cbar=False, cmap='viridis')\n",
    "# g = sns.heatmap(df_X.loc[df_X.isnull().sum(1).sort_values(ascending=1).index].isnull(), cbar=False, cmap='viridis')\n",
    "g.set_xlabel('Column Number')\n",
    "g.set_ylabel('Sample Number')\n",
    "\n",
    "#Decision to be done first with columns : which columns to drop\n",
    "#Then next look at rows and see which rows to drop\n",
    "\n",
    "msno.matrix(df, labels=True, sort='descending', color=(0.27, 0.52, 1.0));\n",
    "\n",
    "#Objective is to elimiate large holes i.e. drop columns which have lots of missing values\n",
    "#Good idea maybe to keep all rows and columns and get a baseline model working. Iterate later to drop rows and columns and check the model performance again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88ui-ZmOfd6O"
   },
   "outputs": [],
   "source": [
    "# Make a decision... drop rows that are 20% or more empty (you set the threshhold)\n",
    "df = df.dropna(thresh=df.shape[1] * 0.80, axis = 0).reset_index(drop=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xox0hRhWfe1G"
   },
   "outputs": [],
   "source": [
    "# c) Missing Values per Feature (Big Holes)\n",
    "df.isna().mean().sort_values().plot(\n",
    "    kind=\"bar\", figsize=(15, 4),\n",
    "    title=\"Percentage of missing values per feature\",\n",
    "    ylabel=\"Ratio of missing values per feature\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p843moq5fffm"
   },
   "outputs": [],
   "source": [
    "# drop any col that is more than 15% empty\n",
    "df = df.dropna(thresh=df.shape[0] * 0.85,axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7lAdIAPgUBu"
   },
   "outputs": [],
   "source": [
    "# d) Impute Values (Small Holes)\n",
    "# df['CustomerID'].fillna('Missing', inplace=True)\n",
    "\n",
    "# Replace NaN one column with the median\n",
    "# df['col1'] = df['col1'].fillna(df['col1'].median())\n",
    "# df = df.fillna(df.median())\n",
    "\n",
    "# to see categorical encoding - see:  https://github.com/fenago/eda/blob/main/Cars_XGBoost.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IPaUxTsbgT4m"
   },
   "outputs": [],
   "source": [
    "# 4. Content Investigation of the Data\n",
    "# a) Distributions of data in each feature\n",
    "# Plots the histogram for each numerical feature in a separate subplot\n",
    "df.hist(bins=25, figsize=(15, 25), layout=(-1, 5), edgecolor=\"black\")\n",
    "plt.tight_layout();\n",
    "\n",
    "#look at columns that are boolean, have only one value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "najnh38JgTv-"
   },
   "outputs": [],
   "source": [
    "# b) Patterns\n",
    "# Creates mask to identify numerical features with more or less than 25 unique features\n",
    "cols_continuous = df.select_dtypes(include=\"number\").nunique() >= 25\n",
    "\n",
    "# Create a new dataframe which only contains the continuous features\n",
    "df_continuous = df[cols_continuous[cols_continuous].index]\n",
    "df_continuous.shape\n",
    "\n",
    "sns.pairplot(df_continuous, height=1.5, plot_kws={\"s\": 2, \"alpha\": 0.2});\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2nZcO-npgTmH"
   },
   "outputs": [],
   "source": [
    "# Create a new dataframe which doesn't contain the numerical continuous features\n",
    "df_discrete = df[cols_continuous[~cols_continuous].index]\n",
    "df_discrete.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFDnrtHmghHm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4X88NpDkghAP"
   },
   "outputs": [],
   "source": [
    "# Feature Relationships\n",
    "# Evaluate but remember to consider multicollinearity\n",
    "\n",
    "# Computes feature correlation\n",
    "df_corr = df.corr(method=\"spearman\") # pearson assumes a linear relationship... spearman does not\n",
    "\n",
    "# Create labels for the correlation matrix\n",
    "labels = np.where(np.abs(df_corr)>0.75, \"S\",\n",
    "                  np.where(np.abs(df_corr)>0.5, \"M\",\n",
    "                           np.where(np.abs(df_corr)>0.25, \"W\", \"\")))\n",
    "\n",
    "# Plot correlation matrix\n",
    "plt.figure(figsize=(15, 15))\n",
    "sns.heatmap(df_corr, mask=np.eye(len(df_corr)), square=True,\n",
    "            center=0, annot=labels, fmt='', linewidths=.5,\n",
    "            cmap=\"vlag\", cbar_kws={\"shrink\": 0.8});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nYzcsa5gg2v"
   },
   "outputs": [],
   "source": [
    "#  Creates a mask to remove the diagonal and the upper triangle.\n",
    "lower_triangle_mask = np.tril(np.ones(df_corr.shape), k=-1).astype(\"bool\")\n",
    "\n",
    "#  Stack all correlations, after applying the mask\n",
    "df_corr_stacked = df_corr.where(lower_triangle_mask).stack().sort_values()\n",
    "\n",
    "#  Showing the lowest and highest correlations in the correlation matrix\n",
    "display(df_corr_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb3nrCumfgJm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqvG8K3seyKm"
   },
   "outputs": [],
   "source": [
    "# # c) Split into Numeric List and Categorical List\n",
    "# # Split the data into numeric and categorical lists and dataframes\n",
    "# numerics = ['int16','int32','int64','float64']\n",
    "# catDF = df.select_dtypes(exclude=numerics) #THESE WOULD BE OHE LATER\n",
    "# numDF = df.select_dtypes(include=numerics) #THESE WOULD BE NORMALIZE/STANDARDIZE LATER\n",
    "# catDF.head()\n",
    "# numDF.head()\n",
    "# # Display numerical features --> numeric features\n",
    "# df.select_dtypes(include=\"number\").head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMXuRvW0f/v2NBuvNx7ZIPi",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
